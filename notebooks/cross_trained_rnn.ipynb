{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SPY'\n",
    "train_end_date = '2022-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tech stock data\n",
    "name_ohlc_df = yf.download(name, start='2012-01-01', end=train_end_date)\n",
    "spy_ohlc_df = yf.download('SPY', start='2012-01-01', end=train_end_date)\n",
    "msft_ohlc_df = yf.download('MSFT', start='2012-01-01', end=train_end_date)\n",
    "goog_ohlc_df = yf.download('GOOG', start='2012-01-01', end=train_end_date)\n",
    "aapl_ohlc_df = yf.download('AAPL', start='2012-01-01', end=train_end_date)\n",
    "meta_ohlc_df = yf.download('META', start='2012-01-01', end=train_end_date)\n",
    "amzn_ohlc_df = yf.download('AMZN', start='2012-01-01', end=train_end_date)\n",
    "nvda_ohlc_df = yf.download('NVDA', start='2012-01-01', end=train_end_date)\n",
    "intc_ohlc_df = yf.download('INTC', start='2012-01-01', end=train_end_date)\n",
    "amd_ohlc_df = yf.download('AMD', start='2012-01-01', end=train_end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df,cols_to_use):\n",
    "    updated_df = df.copy()\n",
    "\n",
    "    updated_df = updated_df[cols_to_use] \n",
    "    upd_scaler = StandardScaler()\n",
    "    upd_scaler = upd_scaler.fit(updated_df)\n",
    "    updated_df_scaled = upd_scaler.transform(updated_df)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "    global n_past\n",
    "    n_past = 14  # Number of past days we want to use to predict the future.\n",
    "    for i in range(n_past, len(updated_df_scaled) - n_future + 1,n_future):\n",
    "        X.append(updated_df_scaled[i - n_past:i, 0:updated_df_scaled.shape[1]])\n",
    "        Y.append(updated_df_scaled[i:i + n_future, 3]) #index 3 is Close\n",
    "\n",
    "    return np.array(X), np.array(Y), upd_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training data\n",
    "df = name_ohlc_df.copy()\n",
    "df = df.reset_index().rename(columns={'index': 'Date'})\n",
    "date_array = df['Date'].to_numpy()\n",
    "date_series = df['Date']\n",
    "cols = list(df)[1:]\n",
    "print(cols) #['Open', 'High', 'Low', 'Close', 'Adj Clos\n",
    "df_for_training = df[cols].astype(float)\n",
    "df_for_training1 = spy_ohlc_df[cols].astype(float)\n",
    "df_for_training2 = msft_ohlc_df[cols].astype(float)\n",
    "df_for_training3 = aapl_ohlc_df[cols].astype(float)\n",
    "df_for_training4 = nvda_ohlc_df[cols].astype(float)\n",
    "\n",
    "columns_to_use = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "trainX, trainY, train_scaler = prepare_dataframe(df_for_training,columns_to_use)\n",
    "trainX1, trainY1, train_scaler = prepare_dataframe(df_for_training1,columns_to_use)\n",
    "trainX2, trainY2, train_scaler = prepare_dataframe(df_for_training2,columns_to_use)\n",
    "trainX3, trainY3, train_scaler = prepare_dataframe(df_for_training3,columns_to_use)\n",
    "trainX4, trainY4, train_scaler = prepare_dataframe(df_for_training4,columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    lstm_output1 = tf.keras.layers.LSTM(128, return_sequences=True)(inputs)\n",
    "    dropout_output1 = tf.keras.layers.Dropout(0.3)(lstm_output1)  # Adding dropout with a rate of 0.2\n",
    "    lstm_output2 = tf.keras.layers.LSTM(128, return_sequences=True)(dropout_output1)\n",
    "    dropout_output2 = tf.keras.layers.Dropout(0.3)(lstm_output2)  \n",
    "    lstm_output3 = tf.keras.layers.LSTM(128, return_sequences=False)(dropout_output2)\n",
    "    dropout_output3 = tf.keras.layers.Dropout(0.3)(lstm_output3)  \n",
    "\n",
    "    flat_mean_output = tf.keras.layers.Dense(5, activation='relu')(dropout_output3) \n",
    "    output_mean = tf.keras.layers.Dense(1)(flat_mean_output)  # Output for mean prediction\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output_mean)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_model(input_shape=(n_past, len(columns_to_use)))\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='mse')\n",
    "big_train_X = np.concatenate((trainX1,trainX2,trainX3,trainX4,trainX),axis=0)\n",
    "big_train_Y = np.concatenate((trainY1,trainY2,trainY3,trainY4,trainY),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(big_train_X, big_train_Y, epochs=20, batch_size=16, validation_split=0.3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = build_lstm_model(input_shape=(n_past, len(columns_to_use)))\n",
    "small_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = small_model.fit(trainX, trainY, epochs=15, batch_size=16, validation_split=0.3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(history.history['loss'],'--', label='Train loss TL')\n",
    "plt.plot(history.history['val_loss'],'--', label='Val lossTL')\n",
    "plt.plot(history2.history['loss'],'--', label='Train loss')\n",
    "plt.plot(history2.history['val_loss'],'--', label='Val loss')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize='x-small')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_end_date)\n",
    "forecast_df = yf.download(name, start=train_end_date, end='2023-07-01')\n",
    "forecast_df = forecast_df.reset_index().rename(columns={'index': 'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_date_array = forecast_df['Date'].to_numpy()[n_past:]\n",
    "truth_opens = forecast_df['Open'].values[n_past:]\n",
    "truth_closes = forecast_df['Close'].values[n_past:]\n",
    "\n",
    "testX, testY, scaler = prepare_dataframe(forecast_df,columns_to_use)\n",
    "\n",
    "#predict the open col\n",
    "forecast = model.predict(testX)\n",
    "forecast_copies = np.repeat(forecast,len(columns_to_use),axis=-1)\n",
    "y_pred_model = scaler.inverse_transform(forecast_copies)[:,3]\n",
    "\n",
    "small_forecast = small_model.predict(testX)\n",
    "small_forecast_copies = np.repeat(small_forecast,len(columns_to_use),axis=-1)\n",
    "y_pred_small_model = scaler.inverse_transform(small_forecast_copies)[:,3]\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame({'Date':forecast_date_array,'Close':truth_closes,\n",
    "                              'Close_pred':y_pred_model, 'Close_small_pred':y_pred_small_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True, gridspec_kw={'height_ratios': [4, 1]})\n",
    "\n",
    "ax[0].plot(prediction_df.Date,prediction_df.Close,label='Close',color='black')\n",
    "\n",
    "ax[0].plot(prediction_df.Date,prediction_df.Close_pred,label=f'Close Pred TL',color='firebrick')\n",
    "ax[0].plot(prediction_df.Date,prediction_df.Close_small_pred,label=f'Close Pred',color='seagreen')\n",
    "ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax[0].set_ylabel('price')\n",
    "ax[0].set_title(name +' Stock (Test Set)')\n",
    "ax[0].grid(color=\"0.95\")\n",
    "\n",
    "ax[1].plot(prediction_df.Date, prediction_df.Close_pred - prediction_df.Close, color='firebrick',label='$\\Delta$ (pred - Close)',lw=0.75)\n",
    "ax[1].plot(prediction_df.Date, prediction_df.Close_small_pred - prediction_df.Close, color='seagreen',label='$\\Delta$ (pred - Close)',lw=0.75)\n",
    "ax[1].axhline(0,ls='--',color='black')\n",
    "ax[1].set_ylabel('Difference')\n",
    "ax[1].grid(color=\"0.95\")\n",
    "ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize='x-small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
